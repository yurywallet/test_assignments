{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the task:\n",
    "Build a text classification service.\n",
    "The model on which the service should be based on may be binary or multiclass/multilabel depending on the dataset of your choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical applications of text classification\n",
    "Text is one of the most common types of unstructured data. Analyzing, understanding, organizing, and sorting through text data is a hard and time-consuming task. \n",
    "\n",
    "<p> A few typical applications of text classification technology for companies include:</p>\n",
    "\n",
    "*\tSocial media monitoring\n",
    "*\tBrand monitoring\n",
    "*\tCustomer service\n",
    "*   Email classification\n",
    "*   Documents labeling\n",
    "\n",
    "<br>\n",
    "The fundamental tasks in Natural Language Processing (NLP) can be divided into:  \n",
    "<ul>\n",
    "<li>sentiment analysis (determining whether a text is positive, negative, or neutral), </li>\n",
    "<li>topic labeling, </li>\n",
    "<li>spam detection,</li>\n",
    "<li>intent detection</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Where can text classification be applied at Kuhne-Nagel\n",
    "\n",
    "At the company’s web-page there are two text-input entry point:\n",
    "* [Quote Request](https://onlineservices.kuehne-nagel.com/ac/login?dest=https%3A%2F%2Fonlineservices.kuehne-nagel.com%2Fecom%2Ffa%2Fquote-request)\n",
    "* [Contact form](https://ee.kuehne-nagel.com/en_gb/other-links/contact-us/)\n",
    "\n",
    "\n",
    "\n",
    "Classification of the topic in each of the entry points can create value to the company's performance. For example, defining information from application form such as type of goods to deliver, conditions and term provided by customer in text format will help to better process the application.\n",
    "\n",
    "Based on that, I was searching for a dataset for topic classification. \n",
    "\n",
    "Among other open-source datasets the “news classification” dataset that is available in a Python [sklearn package](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) that will be used in this task to create a topic classification service.\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups, each corresponding to a different topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality metrics\n",
    "* [F1 score](https://en.wikipedia.org/wiki/F1_score)\n",
    "* [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "The goal of preprocessing is to clean text from different kinds of noise and reduce the dimension of the data.\n",
    "\n",
    "Here are some of the approaches of cleaning:\n",
    "\n",
    "* remove contractions\n",
    "* correct misspelled words\n",
    "* remove special characters\n",
    "* replace emoticons by meaning\n",
    "* remove punctuation\n",
    "* remove accents\n",
    "* replace numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_cleaning(df, col_name ):\n",
    "\n",
    "#    to_remove = ['a','to','of','and']\n",
    "    contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "    punct = \"/-'?!.,#$%'()*+-/:;<=>@[\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€×™√²—–&'\n",
    "    punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "    mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', \n",
    "                    'travelling': 'traveling', 'counselling': 'counseling', \n",
    "                    'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', \n",
    "                    'organisation': 'organization', 'wwii': 'world war 2', \n",
    "                    'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', \n",
    "                    'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', \n",
    "                    'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', \n",
    "                    'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', \n",
    "                    'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                    'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', \n",
    "                    '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                    'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', \n",
    "                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                   'b4':'before', 'otw':'on the way', '2moro':'tomorrow', '2mrrw':'tomorrow', '2mrw':'tomorrow'\n",
    "                   , 'tomrw':'tomorrow', '2morrow':'tomorrow', '4u': 'for you'}    \n",
    "    \n",
    "    def text_specific(x):\n",
    "        x=str(x)\n",
    "        #        remove text before Subject\n",
    "        ind=x.find('Subject:')\n",
    "        x=x[ind:]\n",
    "        \n",
    "            \n",
    "        # remove emails\n",
    "        match=re.findall('\\S+@\\S+', x)\n",
    "        for mail_add in match:\n",
    "            x = x.replace(mail_add, ' ')\n",
    "        for mail_add in match:\n",
    "            x = x.replace(mail_add, ' ')\n",
    "\n",
    "        # remove white space    \n",
    "        x=x.strip()\n",
    "        return x  \n",
    "    df[f'{col_name}_clean']=df[col_name].apply(lambda x: text_specific(x))\n",
    "    \n",
    "    def clean_contractions(x, mapping):\n",
    "        specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "        for s in specials:\n",
    "            x = x.replace(s, \"'\")\n",
    "        x = ' '.join([mapping[t] if t in mapping else t for t in x.split(\" \")])\n",
    "        return x\n",
    "    df[f'{col_name}_clean']=df[f'{col_name}_clean'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "  \n",
    "    def clean_text(x):\n",
    "    \n",
    "        x = str(x)            \n",
    "        for punct in \"/-'\":\n",
    "            x = x.replace(punct, ' ')\n",
    "        for punct in '&':\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "        for punct in \"?!.,\\\"#$%'()*+-/:;<=>@[\\]^_`{|}~' + '“”’\":\n",
    "            x = x.replace(punct, ' ')\n",
    "            \n",
    "        #remove white space    \n",
    "        x=x.strip()\n",
    "        return x  \n",
    "    df[f'{col_name}_clean']=df[f'{col_name}_clean'].apply(lambda x: clean_text(x))\n",
    "    \n",
    "  \n",
    "    \n",
    "    def clean_special_chars(x, punct, mapping):\n",
    "        for p in mapping:\n",
    "            x = x.replace(p, mapping[p])\n",
    "        \n",
    "        for p in punct:\n",
    "            x = x.replace(p, f' {p} ')\n",
    "        \n",
    "        specials = {'u200b': ' ', '…': ' ... ', 'ufeff': '', 'करना': '', 'है': ''}  # Other special characters \n",
    "        for s in specials:\n",
    "            x = x.replace(s, specials[s])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    df[f'{col_name}_clean']=df[f'{col_name}_clean'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "    \n",
    "    def correct_spelling(x, dic):\n",
    "        for word in dic.keys():\n",
    "            x = x.replace(word, dic[word])\n",
    "        return x\n",
    "    \n",
    "    df[f'{col_name}_clean']=df[f'{col_name}_clean'].apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "    \n",
    "    def clean_numbers(x):\n",
    "    \n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "        return x\n",
    "    df[f'{col_name}_clean']=df[f'{col_name}_clean'].apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    def remove_accented_chars(x):\n",
    "        import unidecode\n",
    "        \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "        x = unidecode.unidecode(x)\n",
    "        return x\n",
    "    df[f'{col_name}_clean']=df[f'{col_name}_clean'].apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: 11314 \n",
      "X_test  size: 7532\n",
      "Distribution by category\n",
      "                 target_name  countInTrain  countInTest\n",
      "0           rec.sport.hockey           600          399\n",
      "1     soc.religion.christian           599          398\n",
      "2            rec.motorcycles           598          398\n",
      "3         rec.sport.baseball           597          397\n",
      "4                  sci.crypt           595          396\n",
      "5                  rec.autos           594          396\n",
      "6                    sci.med           594          396\n",
      "7             comp.windows.x           593          395\n",
      "8                  sci.space           593          394\n",
      "9    comp.os.ms-windows.misc           591          394\n",
      "10           sci.electronics           591          393\n",
      "11  comp.sys.ibm.pc.hardware           590          392\n",
      "12              misc.forsale           585          390\n",
      "13             comp.graphics           584          389\n",
      "14     comp.sys.mac.hardware           578          385\n",
      "15     talk.politics.mideast           564          376\n",
      "16        talk.politics.guns           546          364\n",
      "17               alt.atheism           480          319\n",
      "18        talk.politics.misc           465          310\n",
      "19        talk.religion.misc           377          251\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "np.random.seed(42)\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "c_name='message'\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "def prepare(newsgroups_train):\n",
    "\n",
    "    df=pd.DataFrame(newsgroups_train['data'])\n",
    "    df.columns=[c_name]\n",
    "    df['target']=newsgroups_train['target']\n",
    "    di={}\n",
    "    for i in range(20):\n",
    "        di[i]=newsgroups_train['target_names'][i]\n",
    "    df['target_name']=df['target'].map(di)\n",
    "    \n",
    "    df_d=pd.get_dummies(df['target_name'], dummy_na=False)\n",
    "    \n",
    "    df=pd.concat([df,df_d], axis=1)\n",
    "    return df, di\n",
    "\n",
    "X_test, di=prepare(newsgroups_test)\n",
    "X_train, di=prepare(newsgroups_train)\n",
    "\n",
    "targ=list(di.values())\n",
    "\n",
    "#\n",
    "print(f'X_train size: {X_train.shape[0]} \\nX_test  size: {X_test.shape[0]}')\n",
    "a=X_train['target_name'].value_counts().reset_index()\n",
    "a.columns=['target_name', 'countInTrain']\n",
    "b=X_test['target_name'].value_counts().reset_index()\n",
    "b.columns=['target_name', 'countInTest']\n",
    "a=pd.merge(a,b, on='target_name', how='left')\n",
    "print('Distribution by category')\n",
    "print(a)\n",
    "del a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** ORIGINAL ********************\n",
      "From: keegan-edward@cs.yale.edu (Edward Keegan)\n",
      "Subject: DEC MT 486, Adaptec SCSI, 3COMM conflict\n",
      "Organization: Yale University Computer Science Dept., New Haven, CT 06520-2158\n",
      "Lines: 14\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: thumper.cf.cs.yale.edu\n",
      "\n",
      "\n",
      "I have a DEC NT 486DX33 that has an Adaptec SCSI controller, hard disk\n",
      "and cd-rom drive. When I add a 3COMM Ethernet card (3C503) and reboot\n",
      "the system I receive an error message that a boot device cannot be\n",
      "found. Pull the 3COMM card and reboot, everything is fine. I've moved\n",
      "the controller and 3COMM card to various slots, different positions\n",
      "(slot before the controller, slot after the controller) with the\n",
      "same result. DEC hasn't responded to the problem yet. Any help would\n",
      "be appreciated.\n",
      "-- \n",
      "Edward T. Keegan, Facility Director             E-MAIL: keegan@cs.yale.edu\n",
      "Yale University, Computer Science Department     PHONE: 1-203-432-1254\n",
      "51 Prospect Street, Room 009                       FAX: 1-203-432-0593\n",
      "New Haven, CT 06520\n",
      "\n",
      "******************** CLEANED ********************\n",
      "Subject  DEC MT ###  Adaptec SCSI  3COMM conflict\n",
      "Organization  Yale University Computer Science Dept   New Haven  CT ##### ####\n",
      "Lines  ##\n",
      "Distribution  world\n",
      "NNTP Posting Host  thumper cf cs yale edu\n",
      "\n",
      "\n",
      "I have a DEC NT ###DX## that has an Adaptec SCSI controller  hard disk\n",
      "and cd rom drive  When I add a 3COMM Ethernet card  3C###  and reboot\n",
      "the system I receive an error message that a boot device cannot be\n",
      "found  Pull the 3COMM card and reboot  everything is fine  I have moved\n",
      "the controller and 3COMM card to various slots  different positions\n",
      " slot before the controller  slot after the controller  with the\n",
      "same result  DEC has not responded to the problem yet  Any help would\n",
      "be appreciated \n",
      "   \n",
      "Edward T  Keegan  Facility Director             E MAIL   \n",
      "Yale University  Computer Science Department     PHONE  1 ### ### ####\n",
      "## Prospect Street  Room ###                       FAX  1 ### ### ####\n",
      "New Haven  CT #####\n"
     ]
    }
   ],
   "source": [
    "#cleaning and preprocessing\n",
    "X_train=multi_step_cleaning(X_train, c_name )\n",
    "X_test=multi_step_cleaning(X_test, c_name )\n",
    "# print sample\n",
    "print('*'*20 ,'ORIGINAL', '*'*20)\n",
    "print(X_train.loc[112, f'{c_name}'])\n",
    "print('*'*20 ,'CLEANED', '*'*20)\n",
    "print(X_train.loc[112, f'{c_name}_clean'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Machine Learning Approach: Simple rule-based classifier\n",
    "\n",
    "Let's create super simple rule-based classifier for one of categories as a baseline to assess quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length: 210\n"
     ]
    }
   ],
   "source": [
    "w_auto=['car', 'auto', 'automobile', 'vehicle', 'bus', 'truck', 'lorry',\n",
    "        'transport']\n",
    "#car types\n",
    "w_auto+=['pickup','four-wheeler', 'convertible', 'suv', 'sedan','hatchback', 'limousine', 'limo',\n",
    "         'van', 'taxi']\n",
    "#car make\n",
    "w_auto+=['daimler' ,'acura','alfa romeo','audi','bmw','bentley','buick','cadillac','chevrolet','chrysler','dodge','fiat','ford','gmc','genesis','honda','hyundai','infiniti','jaguar','jeep','kia','land rover','lexus','lincoln','lotus','maserati','mazda','mercedes benz','mercedes-benz','mercury','mini cooper','mitsubishi','nissan','polestar','pontiac','porsche','ram','rolls-royce','saab','saturn','scion','smart','subaru','suzuki','tesla','toyota','volkswagen','volvo','geely']\n",
    "#driver \n",
    "w_auto+=['chauffeur','motorist','automobilist']\n",
    "w_auto_s=[f'{x}s' for x in w_auto]\n",
    "w_auto_es=[f'{x}es' for x in w_auto]\n",
    "w_auto+=w_auto_s+w_auto_es\n",
    "del w_auto_s,w_auto_es\n",
    "print(f'Dictionary length: {len(w_auto)}')\n",
    "\n",
    "a=X_test[[f'{c_name}_clean',  'rec.autos']].copy()\n",
    "\n",
    "#predict\n",
    "a['predict']=a[f'{c_name}_clean'].apply(lambda x: 1 if any((' ' + w + ' ') in x for w in w_auto) else 0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Results of the classification ***\n",
      "Total cases from class:\n",
      "Original: 396\n",
      "Predicted: 811\n",
      "      0    1\n",
      "0  6606  530\n",
      "1   115  281 \n",
      "\n",
      " F1:  0.4656 \n",
      " AUC: 0.8177\n"
     ]
    }
   ],
   "source": [
    "def cm2df(cm, labels=[0,1]):\n",
    "    df = pd.DataFrame()\n",
    "    # rows\n",
    "    for i, row_label in enumerate(labels):\n",
    "        rowdata={}\n",
    "        # columns\n",
    "        for j, col_label in enumerate(labels): \n",
    "            rowdata[col_label]=cm[i,j]\n",
    "        df = df.append(pd.DataFrame.from_dict({row_label:rowdata}, orient='index'))\n",
    "    return df[labels]\n",
    "\n",
    "def qual(acm):\n",
    "    prec=acm.iloc[1,1]/(acm.iloc[0,1]+acm.iloc[1,1]) #column\n",
    "    recall=acm.iloc[1,1]/(acm.iloc[1,0]+acm.iloc[1,1]) #row\n",
    "    f1=round(2*(prec*recall)/(prec+recall),4)\n",
    "    auc=round(roc_auc_score(a['rec.autos'],a['predict']),4)\n",
    "    return prec, recall, f1, auc\n",
    "\n",
    "acm=cm2df(confusion_matrix(a['rec.autos'],a['predict']), [0,1])\n",
    "prec, recall, f1, auc=qual(acm)\n",
    "print( '*** Results of the classification ***')\n",
    "print(f\"Total cases from class:\\nOriginal: {a['rec.autos'].sum()}\\nPredicted: {a['predict'].sum()}\")\n",
    "print(acm,'\\n\\n', f'F1:  {round(f1,4)} \\n AUC: {auc}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "\n",
    "Neural Networks with preprained Embeddings are the most common algorythms applied to text classification.\n",
    "Neural network embeddings are useful because they can reduce the dimensionality of categorical variables and meaningfully represent categories in the transformed space (words with similar meaning will be represented with close values).\n",
    "\n",
    "For the purpose to develope a classifier the tiniest embedding 'Glove' was chosen ([Link to Stanford](http://nlp.stanford.edu/data/glove.6B.zip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Preprocessing [link](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)\n",
    "\n",
    "Embeddings\n",
    "* [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)\n",
    "* [glove.840B.300d](https://nlp.stanford.edu/projects/glove/)\n",
    "* [paragram_300_sl999](https://cogcomp.org/page/resource_view/106)\n",
    "* [wiki-news-300d-1M](https://fasttext.cc/docs/en/english-vectors.html)\n",
    "\n",
    "Or \n",
    "\n",
    "https://www.kaggle.com/c/quora-insincere-questions-classification/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yury\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize\n",
      "create weights matrix\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "max_features = 30000\n",
    "embed_size = 300\n",
    "maxlen = 200\n",
    "\n",
    "\n",
    "print(\"tokenize\")\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train['message'].fillna(\"fillna\") ) + list(X_test['message'].fillna(\"fillna\").values))\n",
    "\n",
    "filename = 'tokenizer.sav'\n",
    "pickle.dump(tokenizer, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "X_tr = tokenizer.texts_to_sequences(X_train['message'])\n",
    "X_te = tokenizer.texts_to_sequences(X_test['message'])\n",
    "\n",
    "X_tr = sequence.pad_sequences(X_tr, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(X_te, maxlen=maxlen)\n",
    "\n",
    "EMBEDDING_FILE = 'glove.6B.300d.txt'\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE,encoding='utf-8'))\n",
    "\n",
    "print(\"create weights matrix\")\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, Dropout, Activation,  Conv1D, GlobalMaxPool1D\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n",
    "\n",
    "def get_model(emb=True):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    if emb==True:\n",
    "        x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    else:\n",
    "        x = Embedding(max_features, embed_size)(inp)\n",
    "\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description\n",
    "\n",
    "- Layer 1: Input - As an input model is feeded by result of pad_sequences function. Pad_sequences applied to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length \n",
    "- Layer 2: Embedding. Another input layer to set initial weight to each token (word)\n",
    "- Layer 3: Dropout layers are used to prevent overfitting\n",
    "- Layer 4: Bidirectional (recurent layer) This layers are included because there is an evidence that the context of the whole context is used to interpret what is being \"said\" rather than a linear interpretation\n",
    "- Layer 6: The result is a concatenation of two results of Pooling. Pooling is used to reduce the dimention by summarizing the presence of features in patches. Max pooling gets maximum from the patch. This also helps to reduce the impact of the location of the feature (words) in the input\n",
    "- Layer 7: Output (Dense): classic fully connected neural network layer: each input node is connected to each output node. The result with activation \"sigmoid\" will be a probability that an input belongs to a given class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "There could be \n",
    "\n",
    "- one model trained for all classes \n",
    "- single model trained for each class \n",
    "\n",
    "First approach will be faster but second can provides better quality\n",
    "\n",
    "There are two option to chose from: \n",
    "\n",
    "- single model for each class (train-validation split validation)\n",
    "- Kfold validation (will create K models for each class, the resulting probability will be averaged, and the result will be in general more stable)\n",
    "\n",
    "Both options are implemented but the result will be provided by default \"single model per class\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "\n",
      "Model for alt.atheism\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 211s 23ms/step - loss: 0.1277 - acc: 0.9582 - val_loss: 0.0666 - val_acc: 0.9744\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.975436 \n",
      "\n",
      "predict\n",
      "alt.atheism fold 1 AUC validation: 0.9754\n",
      "alt.atheism fold 1 AUC predict: 0.955\n",
      "F1 score at threshold 0.25 is 0.73\n",
      "******************************\n",
      "\n",
      "Model for comp.graphics\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 204s 23ms/step - loss: 0.1510 - acc: 0.9501 - val_loss: 0.1111 - val_acc: 0.9638\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.956297 \n",
      "\n",
      "predict\n",
      "comp.graphics fold 1 AUC validation: 0.9563\n",
      "comp.graphics fold 1 AUC predict: 0.9455\n",
      "F1 score at threshold 0.125 is 0.687\n",
      "******************************\n",
      "\n",
      "Model for comp.os.ms-windows.misc\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 218s 24ms/step - loss: 0.1508 - acc: 0.9458 - val_loss: 0.0912 - val_acc: 0.9633\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.973142 \n",
      "\n",
      "predict\n",
      "comp.os.ms-windows.misc fold 1 AUC validation: 0.9731\n",
      "comp.os.ms-windows.misc fold 1 AUC predict: 0.9705\n",
      "F1 score at threshold 0.3 is 0.6519\n",
      "******************************\n",
      "\n",
      "Model for comp.sys.ibm.pc.hardware\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 224s 25ms/step - loss: 0.1588 - acc: 0.9473 - val_loss: 0.0997 - val_acc: 0.9611\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.964245 \n",
      "\n",
      "predict\n",
      "comp.sys.ibm.pc.hardware fold 1 AUC validation: 0.9642\n",
      "comp.sys.ibm.pc.hardware fold 1 AUC predict: 0.9549\n",
      "F1 score at threshold 0.2 is 0.6265\n",
      "******************************\n",
      "\n",
      "Model for comp.sys.mac.hardware\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 228s 25ms/step - loss: 0.1494 - acc: 0.9536 - val_loss: 0.0819 - val_acc: 0.9717\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.969902 \n",
      "\n",
      "predict\n",
      "comp.sys.mac.hardware fold 1 AUC validation: 0.9699\n",
      "comp.sys.mac.hardware fold 1 AUC predict: 0.9633\n",
      "F1 score at threshold 0.175 is 0.7373\n",
      "******************************\n",
      "\n",
      "Model for comp.windows.x\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 229s 25ms/step - loss: 0.1308 - acc: 0.9588 - val_loss: 0.0600 - val_acc: 0.9797\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.986748 \n",
      "\n",
      "predict\n",
      "comp.windows.x fold 1 AUC validation: 0.9867\n",
      "comp.windows.x fold 1 AUC predict: 0.9808\n",
      "F1 score at threshold 0.425 is 0.8216\n",
      "******************************\n",
      "\n",
      "Model for misc.forsale\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 231s 26ms/step - loss: 0.1179 - acc: 0.9618 - val_loss: 0.0700 - val_acc: 0.9748\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.974921 \n",
      "\n",
      "predict\n",
      "misc.forsale fold 1 AUC validation: 0.9749\n",
      "misc.forsale fold 1 AUC predict: 0.9804\n",
      "F1 score at threshold 0.175 is 0.7782\n",
      "******************************\n",
      "\n",
      "Model for rec.autos\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 233s 26ms/step - loss: 0.1396 - acc: 0.9526 - val_loss: 0.0618 - val_acc: 0.9792\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.972932 \n",
      "\n",
      "predict\n",
      "rec.autos fold 1 AUC validation: 0.9729\n",
      "rec.autos fold 1 AUC predict: 0.9825\n",
      "F1 score at threshold 0.35 is 0.8197\n",
      "******************************\n",
      "\n",
      "Model for rec.motorcycles\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 236s 26ms/step - loss: 0.1070 - acc: 0.9683 - val_loss: 0.0480 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989586 \n",
      "\n",
      "predict\n",
      "rec.motorcycles fold 1 AUC validation: 0.9896\n",
      "rec.motorcycles fold 1 AUC predict: 0.9903\n",
      "F1 score at threshold 0.7 is 0.8596\n",
      "******************************\n",
      "\n",
      "Model for rec.sport.baseball\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 241s 27ms/step - loss: 0.0815 - acc: 0.9735 - val_loss: 0.0310 - val_acc: 0.9916\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.997738 \n",
      "\n",
      "predict\n",
      "rec.sport.baseball fold 1 AUC validation: 0.9977\n",
      "rec.sport.baseball fold 1 AUC predict: 0.992\n",
      "F1 score at threshold 0.7 is 0.927\n",
      "******************************\n",
      "\n",
      "Model for rec.sport.hockey\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 241s 27ms/step - loss: 0.0872 - acc: 0.9716 - val_loss: 0.0184 - val_acc: 0.9938\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.994482 \n",
      "\n",
      "predict\n",
      "rec.sport.hockey fold 1 AUC validation: 0.9945\n",
      "rec.sport.hockey fold 1 AUC predict: 0.9982\n",
      "F1 score at threshold 0.475 is 0.9451\n",
      "******************************\n",
      "\n",
      "Model for sci.crypt\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 243s 27ms/step - loss: 0.1112 - acc: 0.9653 - val_loss: 0.0305 - val_acc: 0.9898\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.995297 \n",
      "\n",
      "predict\n",
      "sci.crypt fold 1 AUC validation: 0.9953\n",
      "sci.crypt fold 1 AUC predict: 0.9815\n",
      "F1 score at threshold 0.7 is 0.9106\n",
      "******************************\n",
      "\n",
      "Model for sci.electronics\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 246s 27ms/step - loss: 0.1608 - acc: 0.9509 - val_loss: 0.0943 - val_acc: 0.9660\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.965161 \n",
      "\n",
      "predict\n",
      "sci.electronics fold 1 AUC validation: 0.9652\n",
      "sci.electronics fold 1 AUC predict: 0.8944\n",
      "F1 score at threshold 0.15 is 0.6983\n",
      "******************************\n",
      "\n",
      "Model for sci.med\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 252s 28ms/step - loss: 0.1159 - acc: 0.9628 - val_loss: 0.0384 - val_acc: 0.9885\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.991491 \n",
      "\n",
      "predict\n",
      "sci.med fold 1 AUC validation: 0.9915\n",
      "sci.med fold 1 AUC predict: 0.9797\n",
      "F1 score at threshold 0.325 is 0.8996\n",
      "******************************\n",
      "\n",
      "Model for sci.space\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 257s 28ms/step - loss: 0.1227 - acc: 0.9566 - val_loss: 0.0431 - val_acc: 0.9885\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.987661 \n",
      "\n",
      "predict\n",
      "sci.space fold 1 AUC validation: 0.9877\n",
      "sci.space fold 1 AUC predict: 0.9917\n",
      "F1 score at threshold 0.475 is 0.8889\n",
      "******************************\n",
      "\n",
      "Model for soc.religion.christian\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 264s 29ms/step - loss: 0.1184 - acc: 0.9559 - val_loss: 0.0730 - val_acc: 0.9700\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.984858 \n",
      "\n",
      "predict\n",
      "soc.religion.christian fold 1 AUC validation: 0.9849\n",
      "soc.religion.christian fold 1 AUC predict: 0.9864\n",
      "F1 score at threshold 0.275 is 0.7325\n",
      "******************************\n",
      "\n",
      "Model for talk.politics.guns\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 265s 29ms/step - loss: 0.1098 - acc: 0.9667 - val_loss: 0.0453 - val_acc: 0.9850\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.989203 \n",
      "\n",
      "predict\n",
      "talk.politics.guns fold 1 AUC validation: 0.9892\n",
      "talk.politics.guns fold 1 AUC predict: 0.9787\n",
      "F1 score at threshold 0.525 is 0.8451\n",
      "******************************\n",
      "\n",
      "Model for talk.politics.mideast\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 269s 30ms/step - loss: 0.0816 - acc: 0.9762 - val_loss: 0.0324 - val_acc: 0.9920\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.991879 \n",
      "\n",
      "predict\n",
      "talk.politics.mideast fold 1 AUC validation: 0.9919\n",
      "talk.politics.mideast fold 1 AUC predict: 0.9829\n",
      "F1 score at threshold 0.25 is 0.9321\n",
      "******************************\n",
      "\n",
      "Model for talk.politics.misc\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 272s 30ms/step - loss: 0.1253 - acc: 0.9646 - val_loss: 0.0631 - val_acc: 0.9739\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.981701 \n",
      "\n",
      "predict\n",
      "talk.politics.misc fold 1 AUC validation: 0.9817\n",
      "talk.politics.misc fold 1 AUC predict: 0.9134\n",
      "F1 score at threshold 0.25 is 0.6935\n",
      "******************************\n",
      "\n",
      "Model for talk.religion.misc\n",
      "1\n",
      "fit\n",
      "Train on 9051 samples, validate on 2263 samples\n",
      "Epoch 1/1\n",
      "9051/9051 [==============================] - 275s 30ms/step - loss: 0.1235 - acc: 0.9639 - val_loss: 0.0785 - val_acc: 0.9717\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.966996 \n",
      "\n",
      "predict\n",
      "talk.religion.misc fold 1 AUC validation: 0.967\n",
      "talk.religion.misc fold 1 AUC predict: 0.9228\n",
      "F1 score at threshold 0.25 is 0.5083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "folds=5\n",
    "s=4732\n",
    "df_qual=pd.DataFrame()\n",
    "df_threshold=pd.DataFrame()\n",
    "for t in targ:\n",
    "    print('*'*30)\n",
    "    print(f'\\nModel for {t}')\n",
    "    fol=0\n",
    "    y_tr=X_train[t]\n",
    "    y_te=X_test[t]\n",
    "    \n",
    "    mdl = get_model(True)\n",
    "    #print(model.summary())\n",
    "    qq=[]\n",
    "    thr=[]\n",
    "    kfold_model=0\n",
    "    if kfold_model==0:\n",
    "        fol+=1\n",
    "        print(fol)\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(X_tr, y_tr, train_size=0.80, random_state=233, stratify=y_tr)\n",
    "        \n",
    "        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "        print(\"fit\")\n",
    "        mdll = mdl.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                         callbacks=[RocAuc]\n",
    "        #                 , verbose=0\n",
    "                         )\n",
    "        print(\"predict\")\n",
    "        y_pred_val = mdl.predict(X_val, batch_size=1024)\n",
    "        y_pred = mdl.predict(X_te, batch_size=1024)\n",
    "\n",
    "        auc=round(roc_auc_score(y_val,y_pred_val),4)\n",
    "        print(f'{t} fold {fol} AUC validation: {auc}')\n",
    "        qq.append(auc)\n",
    "\n",
    "\n",
    "        #f1=round(f1_score(y_te,y_pred),4)\n",
    "        auc=round(roc_auc_score(y_te,y_pred),4)\n",
    "        print(f'{t} fold {fol} AUC predict: {auc}')\n",
    "        filename = f'finalized_model_{t}_{fol}.sav'\n",
    "        pickle.dump(mdl, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "        fscor=0\n",
    "        thresh_optima=0.5\n",
    "        for thresh in np.arange(0.1, 0.701, 0.025):\n",
    "            thresh = np.round(thresh, 3)\n",
    "            fs=f1_score(y_val, (y_pred_val>thresh).astype(int))\n",
    "            if fs>=fscor: \n",
    "                thresh_optima=thresh\n",
    "                fscor=fs\n",
    "        thr.append(thresh_optima)\n",
    "        print(f\"F1 score at threshold {thresh_optima} is {round(fscor,4)}\")\n",
    "\n",
    "        #store quality\n",
    "        df_qual[t]=qq  \n",
    "        #store threshold\n",
    "        df_threshold[t]= thr  \n",
    "    \n",
    "    else:\n",
    "    \n",
    "        skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=s)\n",
    "        sk_f=skf.split(X_tr,  y_tr)\n",
    "\n",
    "\n",
    "        for train_index, test_index in sk_f:\n",
    "            fol+=1\n",
    "            print(fol)\n",
    "            #    X_tra, X_val, y_tra, y_val = train_test_split(X_tr, y_tr, train_size=0.80, random_state=233, stratify=y_tr)\n",
    "\n",
    "            X_tra=X_tr[train_index] \n",
    "            X_val=X_tr[test_index]\n",
    "\n",
    "            y_tra=y_tr[train_index] \n",
    "            y_val=y_tr[test_index]\n",
    "\n",
    "            RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "            print(\"fit\")\n",
    "            mdll = mdl.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                             callbacks=[RocAuc]\n",
    "            #                 , verbose=0\n",
    "                             )\n",
    "            print(\"predict\")\n",
    "            y_pred_val = mdl.predict(X_val, batch_size=1024)\n",
    "            y_pred = mdl.predict(X_te, batch_size=1024)\n",
    "\n",
    "            auc=round(roc_auc_score(y_val,y_pred_val),4)\n",
    "            print(f'{t} fold {fol} AUC validation: {auc}')\n",
    "            qq.append(auc)\n",
    "\n",
    "\n",
    "            #f1=round(f1_score(y_te,y_pred),4)\n",
    "            auc=round(roc_auc_score(y_te,y_pred),4)\n",
    "            print(f'{t} fold {fol} AUC predict: {auc}')\n",
    "            filename = f'finalized_model_{t}_{fol}.sav'\n",
    "            pickle.dump(mdl, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "            fscor=0\n",
    "            thresh_optima=0.5\n",
    "            for thresh in np.arange(0.1, 0.701, 0.025):\n",
    "                thresh = np.round(thresh, 3)\n",
    "                fs=f1_score(y_val, (y_pred_val>thresh).astype(int))\n",
    "                if fs>=fscor: \n",
    "                    thresh_optima=thresh\n",
    "                    fscor=fs\n",
    "            thr.append(thresh_optima)\n",
    "            print(f\"F1 score at threshold {thresh_optima} is {round(fscor,4)}\")\n",
    "\n",
    "        #store quality\n",
    "        df_qual[t]=qq  \n",
    "        #store threshold\n",
    "        df_threshold[t]= thr  \n",
    "    \n",
    "df_threshold.to_csv(\"threshold.csv\")\n",
    "df_qual.to_csv(\"qual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold=pd.read_csv('threshold.csv')\n",
    "df_qual=pd.read_csv('qual.csv')\n",
    "df_threshold=df_threshold.T.reset_index()\n",
    "df_qual=df_qual.T.reset_index()\n",
    "if kfold_model==0:\n",
    "    df_threshold['mean']=  df_threshold.iloc[:,1]\n",
    "    df_qual['mean']=  df_qual.iloc[:,1]\n",
    "else:\n",
    "    df_threshold['mean']=  df_threshold.iloc[:,1:folds+1].mean(axis=1) \n",
    "    df_qual['mean']=  df_qual.iloc[:,1:folds+1].mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_threshold)\n",
    "#print(df_qual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction( my_text , targ=targ, folds=folds, maxlen=maxlen, kfold_model=0):\n",
    "    if kfold_model==0: folds=1 \n",
    "    rez_prob_vector={}\n",
    "    #preprocessing\n",
    "    X=pd.DataFrame()\n",
    "    X[c_name]=[my_text]\n",
    "    X=multi_step_cleaning(X, c_name)\n",
    "    #print('cleaning done')\n",
    "    # transformation\n",
    "    tokenizer = pickle.load(open('tokenizer.sav', 'rb'))\n",
    "    text_conv = tokenizer.texts_to_sequences(X[f'{c_name}_clean'])\n",
    "    text_conv = sequence.pad_sequences(text_conv, maxlen=maxlen)\n",
    "    #print('token done')  \n",
    "    print(f'Prediction [{len(targ)}]: ', end='')\n",
    "    df=pd.DataFrame(columns=['target','probability']) \n",
    "    for t in targ:\n",
    "        print('|', end='')\n",
    "        rez_prob_vector[t]=0\n",
    "        for i in range(1,folds+1):\n",
    "        #load model\n",
    "            loaded_model = pickle.load(open(f'finalized_model_{t}_{i}.sav', 'rb'))\n",
    "            v=loaded_model.predict(text_conv, batch_size=1024)\n",
    "            rez_prob_vector[t]+=v[0][0]\n",
    "            df.loc[df.shape[0]]=[t,v[0][0]]\n",
    "        #print(rez_prob_vector[t])\n",
    "\n",
    "    df=df.sort_values(['probability'], ascending=[False] )\n",
    "    print (f\"\\n\\nThe most probable topic is: {df.iloc[0,0]}\")                  \n",
    "    print('*'*40)      \n",
    "    print (\"\\nPossible topics of the text are: \")\n",
    "    rez=[]\n",
    "    for t in targ:\n",
    "        if kfold_model==0:\n",
    "            trsh=0.30\n",
    "        else:\n",
    "            trsh=df_threshold.loc[df_threshold['index']==t, 'mean'].sum()\n",
    "        if rez_prob_vector[t]>trsh: rez.append(t)\n",
    "    \n",
    "    if len(rez)==0:\n",
    "        print (f'* general *', end = '')\n",
    "    else:\n",
    "        for i in rez:\n",
    "            print (f'* {i} *', end = '')\n",
    "    print(\"\\n\",\"*\"*40,'\\n')\n",
    " \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98dce2b81cd42c589dca5f50014e626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>Text</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "Text(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction [20]: ||||||||||||||||||||\n",
      "\n",
      "The most probable topic is: comp.sys.ibm.pc.hardware\n",
      "****************************************\n",
      "\n",
      "Possible topics of the text are: \n",
      "* comp.sys.ibm.pc.hardware *\n",
      " **************************************** \n",
      "\n",
      "                      target  probability\n",
      "3   comp.sys.ibm.pc.hardware     0.650590\n",
      "4      comp.sys.mac.hardware     0.097121\n",
      "2    comp.os.ms-windows.misc     0.056079\n",
      "6               misc.forsale     0.025849\n",
      "12           sci.electronics     0.021547\n",
      "13                   sci.med     0.003645\n",
      "5             comp.windows.x     0.002233\n",
      "19        talk.religion.misc     0.002187\n",
      "11                 sci.crypt     0.001585\n",
      "1              comp.graphics     0.001144\n",
      "7                  rec.autos     0.001116\n",
      "0                alt.atheism     0.001108\n",
      "9         rec.sport.baseball     0.000941\n",
      "8            rec.motorcycles     0.000755\n",
      "17     talk.politics.mideast     0.000600\n",
      "15    soc.religion.christian     0.000504\n",
      "14                 sci.space     0.000444\n",
      "16        talk.politics.guns     0.000399\n",
      "10          rec.sport.hockey     0.000353\n",
      "18        talk.politics.misc     0.000326\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "my_text=widgets.Text()\n",
    "display(my_text)\n",
    "\n",
    "def test_textf(mytext):\n",
    "    print (\"**\", mytext, '**')\n",
    "    \n",
    "def handle_submit(sender):\n",
    "    #print('start')\n",
    "    df=prediction(my_text.value)\n",
    "    print(df)\n",
    "#    test_textf( my_text.value)\n",
    "my_text.on_submit(handle_submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "* By implementing Neural Network algorythms with pretrained embedding the task for topic classification can be solved with high accuracy\n",
    "* Based on the quantity of samples in each class from chosen dataset we can expect that by adding extra text samples algorythm will be able to learn new category\n",
    "\n",
    "\n",
    "## Further improvements\n",
    "\n",
    "- find database to be inline with real company needs \n",
    "- extract Subject, Organization, etc. and use as a separate input\n",
    "- test different Embeddings\n",
    "- try other network structures\n",
    "- find ways to make it work faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull links & Datasets\n",
    "\n",
    "* https://nlpprogress.com/english/text_classification.html\n",
    "\n",
    "* https://www.kaggle.com/sbongo/tackling-toxic-problem-with-char-gram-cnn-lstm\n",
    "\n",
    "* [Quora-insincere questions classification](https://www.kaggle.com/c/quora-insincere-questions-classification/overview)\n",
    "* [Movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "* [News categorization](https://www.kaggle.com/yufengdev/bbc-text-categorization)\n",
    "* [SMS spam](https://www.kaggle.com/kredy10/simple-lstm-for-text-classification)\n",
    "* http://www.qizhexie.com/data/RACE_leaderboard.html)\n",
    "* https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e\n",
    "* https://blog.cambridgespark.com/50-free-machine-learning-datasets-natural-language-processing-d88fb9c5c8da\n",
    "* https://lionbridge.ai/datasets/the-best-25-datasets-for-natural-language-processing/\n",
    "* https://aws.amazon.com/ru/datasets/google-books-ngrams/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
